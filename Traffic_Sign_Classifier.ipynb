{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Traffic Sign Recognition Classifier, Deep Learning Approach \n",
    "\n",
    "### Overview  \n",
    "\n",
    "In this project, I train train and validate a CNN model to classify traffic sign images using the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset). Below are the following steps of the pipeline that I have developed:  \n",
    "\n",
    "- Loading the provided raw data, **trainin**, **validation**, and **test** data sets. \n",
    "- exploring the raw data, data set size for each category, visualizing, and identifying how it can be pre=-processed to improve model performance in general \n",
    "- Applying the modifications to the images and storing the processed data to be loaded for the next runs \n",
    "- Defining the CNN model and finetuning hyper-parameters  \n",
    "- Test and visualize the performance on provided (and processed) **test** data set \n",
    "- Visualize itermediate layers' output \n",
    "\n",
    "> **Note**: `Cnn` wrapper class is created alongside this jupyter notebook for better code review and readability. In a sense, it is similar to `Keras` library, but of course a lot more limited functionality, just to cover the requirements of this project.  \n",
    "\n",
    "### 1. Laoding and exploring the provided raw data  \n",
    "\n",
    "- **Whitenning Images**:  \n",
    "    While exploring the raw data, I noticed several images that were dark, not really highlighting the features that the CNN could clearly capture. To improve on it, I developed function `Cnn.whiten_images_self_mean` that takes an image as input and slightly enlighten - or whiten - it with respect to its own average RGB values. That can also be performed on HLS space, increasing the light component. But that has worked well for now. Below you can see some examples of the images before and after whitening.  \n",
    "\n",
    "Before Whitenning             |  After Whitenning\n",
    ":-------------------------:|:-------------------------:\n",
    "![Before](Images/sample-01-d.png)  |  ![After](Images/sample-01.png)  \n",
    " | \n",
    "![Before](Images/sample-02-d.png)  |  ![After](Images/sample-02.png)  \n",
    " |\n",
    "\n",
    "***\n",
    "> **Note**:\n",
    "There are couple of other appraoches proposed in literatures for image whitenning that I tried, but they did not work as expected. One of them that I already tried is called **Zero Components Analysis** or **ZCA** for short. I think this method requires some basic requirements of training data set that are not met here. After exploring some of the whitenned images, the results were so deviated from the original image that one could not identify the original image. But it is still worth exploring... future work  \n",
    "- [LINK-1: Preprocessing for deep learning: from covariance matrix to image whitening](https://hadrienj.github.io/posts/Preprocessing-for-deep-learning/)\n",
    "- [LINK-2: Preprocessing for deep learning: from covariance matrix to image whitening](https://www.freecodecamp.org/news/preprocessing-for-deep-learning-from-covariance-matrix-to-image-whitening-9e2b9c75165c/)  \n",
    "There are also some general useful guidelines that I followed, such as the link below: \n",
    "- [Image Pre-processing for Deep Learning](https://towardsdatascience.com/image-pre-processing-c1aec0be3edf) \n",
    "\n",
    "  \n",
    "\n",
    "- **Data Augmentation**:  \n",
    "    It is observed that the provided raw data does not have enough data for some categories. More importantly, some features have significantly more data than ohers. Here is the distribution of number of images of each category:  \n",
    "    \n",
    "    `features_counts: [ 180 1980 2010 1260 1770 1650  360 1290 1260 1320 1800 1170 1890 1920 690  540  360  990 1080  180  300  270  330 450  240 1350  540  210 480  240  390  690  210  599  360 1080  330  180 1860  270  300  210 210]`\n",
    "    \n",
    "    So it is needed to augment the data in a logical way. My simple approach is to generate some fake images of each category by applying slight noises. Function `Cnn.augment_data` is created to perform this job. After data augmentation, equal sample size of each category is selected form the post-processed and augmented data and the results are saved into the `traffic-signs-data` directory for future uses. \n",
    "    \n",
    "    > **Note**: This task needs to be run only once, therefore for future runs, the post-processed data will be loaded for training, validation, and test steps. \n",
    "    \n",
    "    After training-data augmentation process, each category will have enough number of data as follows. \n",
    "    \n",
    "    `features_counts: [1440, 3240, 3270, 2520, 3030, 2910, 1620, 2550, 2520, 2580, 3060, 2430, 3150, 3180, 1950, 1800, 1620, 2250, 2340, 1440, 1560, 1530, 1590, 1710, 1500, 2610, 1800, 1470, 1740, 1500, 1650, 1950, 1470, 1859, 1620, 2340, 1590, 1440, 3120, 1530, 1560, 1470, 1470]`  \n",
    "        \n",
    "    However, the training data is loaded such that each category has equal sample size as follows:  \n",
    "    \n",
    "    `features_counts: [1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440]`  \n",
    "    \n",
    "### Defining the CNN model:  \n",
    "\n",
    "The model arcitecture consists of five CNN layers followed two fully-connected - FC - layers. The parameters of the CNN model is selected and tuned such that it becomes smallet as it goes deeper, i.e. more feature depths but smaller layers. The output of each CNN layer is fed into a batch-normalizer to ensure that the results - trained weights and biases - do not overshoot, causing numerical instability and poor training performance.  \n",
    "\n",
    "For the FC layers, dropout appraoch is used to prevent overfitting. \n",
    "\n",
    "### Results and Discussions  \n",
    "\n",
    "After going through 50 iterations on the training data batches - `epochs = 50` - the validatoin accuracy of close to `98%` is achieved. Tracking the training-loss suggets that the model performance is resonable and it did not overfit the training data. This is later confirmed by using the **test** data set that is not exposed to the model at all. The overall test accuracy is close to `95%`. \n",
    "\n",
    "### Potential Improvement Areas  \n",
    "\n",
    "I can think of couple of improvement areas that I would like to explore. \n",
    "\n",
    "-  **Utilizing more training data**:  \n",
    "   In general, by adding more clear and clean data of each category, the performance should be improved. Currently, the data augmentation is done by applying some level of noise. However, by adding more cear publicly-available data - that are clean and clear - one may improve the performance.  \n",
    "   \n",
    "-  By examining the wrong predictions, it is observed that some of them are actually from the first 9 categories that are related to speed limit signs. Therefore, One way is to train a CNN model solely for the first 9 categories and the other one for all 43 categories - or just the other 34 categories. The final score will be decided after passing the test images into both CNNs. One benefit is that one may use smaller CNN size to get better speed and accuracy performance.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "from random import shuffle\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from keras.layers import Input, InputLayer, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout\n",
    "from keras.models import Sequential,Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "import keras\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_raw_file = \"traffic-signs-data/train.p\"\n",
    "validation_raw_file = \"traffic-signs-data/valid.p\"\n",
    "testing_raw_file = \"traffic-signs-data/test.p\"\n",
    "\n",
    "training_processed_file = \"traffic-signs-data/train-processed.p\"\n",
    "validation_processed_file = \"traffic-signs-data/valid-processed.p\"\n",
    "testing_processed_file = \"traffic-signs-data/test-processed.p\"\n",
    "\n",
    "csv_features_file = \"signnames.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data explorations and processing    \n",
    "\n",
    "- loading the alread-provided raw data \n",
    "- data augmentation  \n",
    "- image whitenning \n",
    "- storing the processed data - augmented and whitenned \n",
    "\n",
    "> **Note**: This cell needs to be run only once - although it can be run several times, but not really needed. In the following cells below, the generated data file will be loaded and used for training purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features count before augmentation: [ 180 1980 2010 1260 1770 1650  360 1290 1260 1320 1800 1170 1890 1920\n",
      "  690  540  360  990 1080  180  300  270  330  450  240 1350  540  210\n",
      "  480  240  390  690  210  599  360 1080  330  180 1860  270  300  210\n",
      "  210]\n",
      "features count after augmentation: [360, 2160, 2190, 1440, 1950, 1830, 540, 1470, 1440, 1500, 1980, 1350, 2070, 2100, 870, 720, 540, 1170, 1260, 360, 480, 450, 510, 630, 420, 1530, 720, 390, 660, 420, 570, 870, 390, 779, 540, 1260, 510, 360, 2040, 450, 480, 390, 390]\n",
      "features count before augmentation: [ 360 2160 2190 1440 1950 1830  540 1470 1440 1500 1980 1350 2070 2100\n",
      "  870  720  540 1170 1260  360  480  450  510  630  420 1530  720  390\n",
      "  660  420  570  870  390  779  540 1260  510  360 2040  450  480  390\n",
      "  390]\n",
      "features count after augmentation: [720, 2520, 2550, 1800, 2310, 2190, 900, 1830, 1800, 1860, 2340, 1710, 2430, 2460, 1230, 1080, 900, 1530, 1620, 720, 840, 810, 870, 990, 780, 1890, 1080, 750, 1020, 780, 930, 1230, 750, 1139, 900, 1620, 870, 720, 2400, 810, 840, 750, 750]\n",
      "features count before augmentation: [ 720 2520 2550 1800 2310 2190  900 1830 1800 1860 2340 1710 2430 2460\n",
      " 1230 1080  900 1530 1620  720  840  810  870  990  780 1890 1080  750\n",
      " 1020  780  930 1230  750 1139  900 1620  870  720 2400  810  840  750\n",
      "  750]\n",
      "features count after augmentation: [1440, 3240, 3270, 2520, 3030, 2910, 1620, 2550, 2520, 2580, 3060, 2430, 3150, 3180, 1950, 1800, 1620, 2250, 2340, 1440, 1560, 1530, 1590, 1710, 1500, 2610, 1800, 1470, 1740, 1500, 1650, 1950, 1470, 1859, 1620, 2340, 1590, 1440, 3120, 1530, 1560, 1470, 1470]\n",
      "- pre-processing raw images and applying self-mean correction to whiten them... started\n",
      "  completed!           \n",
      "- pre-processing raw images and applying self-mean correction to whiten them... started\n",
      "  completed!           \n",
      "- pre-processing raw images and applying self-mean correction to whiten them... started\n",
      "  completed!           \n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import classificationModules\n",
    "reload(classificationModules)\n",
    "from classificationModules import Cnn\n",
    "\n",
    "# in case resizing is required - not used in this pipeline\n",
    "target_image_size = 32\n",
    "epsilon = 0.0\n",
    "\n",
    "with open(training_raw_file, mode=\"rb\") as f:\n",
    "    train_raw_data = pickle.load(f)\n",
    "with open(validation_raw_file, mode=\"rb\") as f:\n",
    "    valid_raw_data = pickle.load(f)\n",
    "with open(testing_raw_file, mode=\"rb\") as f:\n",
    "    test_raw_data = pickle.load(f)\n",
    "\n",
    "# pre-processing the raw data \n",
    "\n",
    "x_train_raw_data, y_train_raw_data = train_raw_data[\"features\"], train_raw_data[\"labels\"]\n",
    "x_valid_raw_data, y_valid_raw_data = valid_raw_data[\"features\"], valid_raw_data[\"labels\"]\n",
    "x_test_raw_data, y_test_raw_data = test_raw_data[\"features\"], test_raw_data[\"labels\"]\n",
    "\n",
    "x_train_norm_data = Cnn.normalize(x_train_raw_data, approach=\"scale\")\n",
    "x_valid_norm_data = Cnn.normalize(x_valid_raw_data, approach=\"scale\")\n",
    "x_test_norm_data = Cnn.normalize(x_test_raw_data, approach=\"scale\")\n",
    "\n",
    "y_train_norm_data = y_train_raw_data\n",
    "\n",
    "x_train_norm_data, y_train_norm_data, features_counts = Cnn.augment_data(x_train_norm_data, y_train_norm_data, epsilon=0.05)\n",
    "x_train_norm_data, y_train_norm_data, features_counts = Cnn.augment_data(x_train_norm_data, y_train_norm_data, epsilon=-0.05)\n",
    "x_train_norm_data, y_train_norm_data, features_counts = Cnn.augment_data(x_train_norm_data, y_train_norm_data, epsilon=0.05)\n",
    "\n",
    "# Note: ZCA approach is not used for now \n",
    "# Cnn.whiten_images_zca(x_train_norm_data, train_processed_data[\"features\"], batch_size=1000, epsilon=0.1)\n",
    "# Cnn.whiten_images_zca(x_valid_norm_data, valid_processed_data[\"features\"], batch_size=1000, epsilon=0.1)\n",
    "# Cnn.whiten_images_zca(x_test_norm_data, test_processed_data[\"features\"], batch_size=1000, epsilon=0.1)\n",
    "\n",
    "# np.zeros_like(x_train_norm_data).astype(np.uint8)\n",
    "train_processed_data = {\"features\":None, \"labels\":y_train_norm_data}\n",
    "valid_processed_data = {\"features\":None, \"labels\":y_valid_raw_data}\n",
    "test_processed_data = {\"features\":None, \"labels\":y_test_raw_data}\n",
    "\n",
    "train_processed_data[\"features\"] = Cnn.whiten_images_self_mean(x_train_norm_data, batch_size=1000, width_offset=1, height_offset=1, \n",
    "                                                               epsilon=0.0, verbose=True)\n",
    "valid_processed_data[\"features\"] = Cnn.whiten_images_self_mean(x_valid_norm_data, batch_size=1000, width_offset=1, height_offset=1, \n",
    "                                                               epsilon=0.0, verbose=True)\n",
    "test_processed_data[\"features\"] = Cnn.whiten_images_self_mean(x_test_norm_data, batch_size=1000, width_offset=1, height_offset=1, \n",
    "                                                              epsilon=0.0, verbose=True)\n",
    "# Note: useful for debugging only \n",
    "# train_processed_data = np.copy(train_raw_data)\n",
    "# valid_processed_data = np.copy(valid_raw_data)\n",
    "# test_processed_data = np.copy(test_raw_data)\n",
    "\n",
    "with open(training_processed_file, mode=\"wb\") as f:\n",
    "    pickle.dump(train_processed_data, f)\n",
    "with open(validation_processed_file, mode=\"wb\") as f:\n",
    "    pickle.dump(valid_processed_data, f)\n",
    "with open(testing_processed_file, mode=\"wb\") as f:\n",
    "    pickle.dump(test_processed_data, f)\n",
    "\n",
    "# releasing memory \n",
    "\n",
    "# train_raw_data = None\n",
    "valid_raw_data = None\n",
    "test_raw_data = None\n",
    "\n",
    "# train_processed_data = None\n",
    "valid_processed_data = None\n",
    "test_processed_data = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring some of the post-processed images  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x25d533c8940>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 108x108 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 108x108 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_images = train_processed_data[\"features\"]\n",
    "temp_labels = train_processed_data[\"labels\"]\n",
    "\n",
    "test_image_index = 2100\n",
    "\n",
    "plt.figure(figsize=(1.5, 1.5))\n",
    "plt.imshow(temp_images[test_image_index])\n",
    "\n",
    "# =============================================================\n",
    "\n",
    "temp_images = x_train_norm_data\n",
    "temp_labels = y_train_norm_data\n",
    "\n",
    "plt.figure(figsize=(1.5, 1.5))\n",
    "plt.imshow(temp_images[test_image_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data sets from the post-processed pickle files \n",
    "- In this cell, already-preprocessed data files are loaded for training, validation, and test purposes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open(training_raw_file, mode=\"rb\") as f:\\n    train_data = pickle.load(f)\\nwith open(validation_raw_file, mode=\"rb\") as f:\\n    valid_data = pickle.load(f)\\nwith open(testing_raw_file, mode=\"rb\") as f:\\n    test_data = pickle.load(f)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(training_processed_file, mode=\"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open(validation_processed_file, mode=\"rb\") as f:\n",
    "    valid_data = pickle.load(f)\n",
    "with open(testing_processed_file, mode=\"rb\") as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "\"\"\"\n",
    "with open(training_raw_file, mode=\"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "with open(validation_raw_file, mode=\"rb\") as f:\n",
    "    valid_data = pickle.load(f)\n",
    "with open(testing_raw_file, mode=\"rb\") as f:\n",
    "    test_data = pickle.load(f)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training, validation, and test data sets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import classificationModules\n",
    "reload(classificationModules)\n",
    "from classificationModules import Cnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 88979\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 43\n",
      "\n",
      "Features count of training set\n",
      ": [1440, 3240, 3270, 2520, 3030, 2910, 1620, 2550, 2520, 2580, 3060, 2430, 3150, 3180, 1950, 1800, 1620, 2250, 2340, 1440, 1560, 1530, 1590, 1710, 1500, 2610, 1800, 1470, 1740, 1500, 1650, 1950, 1470, 1859, 1620, 2340, 1590, 1440, 3120, 1530, 1560, 1470, 1470]\n",
      "\n",
      "Features count of validation set\n",
      ": [30, 240, 240, 150, 210, 210, 60, 150, 150, 150, 210, 150, 210, 240, 90, 90, 60, 120, 120, 30, 60, 60, 60, 60, 30, 150, 60, 30, 60, 30, 60, 90, 30, 90, 60, 120, 60, 30, 210, 30, 60, 30, 30]\n",
      "\n",
      "Features count of test set\n",
      ": [60, 720, 750, 450, 660, 630, 150, 450, 450, 480, 660, 420, 690, 720, 270, 210, 150, 360, 390, 60, 90, 90, 120, 150, 90, 480, 180, 60, 150, 90, 150, 270, 60, 210, 120, 390, 120, 60, 690, 90, 90, 60, 90]\n",
      "\n",
      "Features counts after equal size selection:\n",
      " [1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440]\n"
     ]
    }
   ],
   "source": [
    "x_train_raw_data, y_train_raw_data = train_data[\"features\"], train_data[\"labels\"]\n",
    "x_valid_raw_data, y_valid_raw_data = valid_data[\"features\"], valid_data[\"labels\"]\n",
    "x_test_raw_data, y_test_raw_data = test_data[\"features\"], test_data[\"labels\"]\n",
    "\n",
    "# in case resizing is required - not used in this pipeline\n",
    "target_image_size = 32\n",
    "    \n",
    "features_df = pd.read_csv(csv_features_file)\n",
    "unique_label_ids = [row[\"ClassId\"] for _, row in features_df.iterrows()]\n",
    "unique_label_names = [row[\"SignName\"] for _, row in features_df.iterrows()]\n",
    "\n",
    "n_train = x_train_raw_data.shape[0]\n",
    "n_validation = x_valid_raw_data.shape[0]\n",
    "n_test = x_test_raw_data.shape[0]\n",
    "\n",
    "# shape of an traffic sign image\n",
    "image_shape = x_train_raw_data.shape[1:4]\n",
    "\n",
    "# unique classes/labels there are in the dataset\n",
    "assert np.array_equal(np.sort(np.unique(y_train_raw_data)), np.sort(np.asarray(unique_label_ids))), \\\n",
    "    \"There is a mismatch in the training data\"\n",
    "\n",
    "n_classes = len(unique_label_ids)\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)\n",
    "\n",
    "# n_classes = len(cnn.label_ids)\n",
    "\n",
    "features_counts = [(np.where(train_data[\"labels\"] == label_id))[0].size for label_id in unique_label_ids]\n",
    "print(\"\\nFeatures count of training set\\n:\", features_counts)\n",
    "print(\"\\nFeatures count of validation set\\n:\", [(np.where(valid_data[\"labels\"] == label_id))[0].size for label_id in unique_label_ids])\n",
    "print(\"\\nFeatures count of test set\\n:\", [(np.where(test_data[\"labels\"] == label_id))[0].size for label_id in unique_label_ids])\n",
    "\n",
    "# =================================================================================================\n",
    "\n",
    "x_train_raw_data, y_train_raw_data, new_features_counts = \\\n",
    "    Cnn.select_equally_sized_data_sets(x_train_raw_data, y_train_raw_data, n_classes=n_classes, \n",
    "                                       unique_label_ids=unique_label_ids, features_counts=features_counts)\n",
    "\n",
    "print(\"\\nFeatures counts after equal size selection:\\n\", new_features_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing loaded data  \n",
    "- Normalization of loaded data between 0 and 1 \n",
    "- One-hot encoding of the loaded labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from importlib import reload\n",
    "import classificationModules\n",
    "reload(classificationModules)\n",
    "from classificationModules import Cnn\n",
    "\n",
    "cnn = Cnn()\n",
    "cnn.init_model(unique_label_names, unique_label_ids)\n",
    "\n",
    "x_train_data, y_train_data = Cnn.normalize(x_train_raw_data, approach=\"scale\"), cnn.one_hot_encode(y_train_raw_data)\n",
    "x_valid_data, y_valid_data = Cnn.normalize(x_valid_raw_data, approach=\"scale\"), cnn.one_hot_encode(y_valid_raw_data)\n",
    "x_test_data, y_test_data = Cnn.normalize(x_test_raw_data, approach=\"scale\"), cnn.one_hot_encode(y_test_raw_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the traning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data = list(zip(x_train_data, y_train_data))\n",
    "random.shuffle(labelled_data)\n",
    "random_x_train_data, random_y_train_data = zip(*labelled_data)\n",
    "\n",
    "random_x_train_data = list(random_x_train_data)\n",
    "random_y_train_data = list(random_y_train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring and visualizing the shuffled training data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "cnn.display_images(random_x_train_data[0:5], random_y_train_data[0:5], normalized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_maxpool... Start\n",
      "Checking inputs dimensions...\n",
      "conv_ksize: (8, 8)\n",
      "conv_num_outputs: 16\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Checking strides dimensions...\n",
      "conv_strides: (1, 1, 1, 1)\n",
      "pool_ksize: (1, 2, 2, 1)\n",
      "pool_strides (1, 1, 1, 1)\n",
      "batch_normalizer: <tensorflow.python.keras.layers.normalization.BatchNormalizationV1 object at 0x0000021D9393EA20>\n",
      "conv_layer: (?, 24, 24, 16)\n",
      "conv2d_maxpool... End\n",
      "\n",
      "conv2d_maxpool... Start\n",
      "Checking inputs dimensions...\n",
      "conv_ksize: (5, 5)\n",
      "conv_num_outputs: 32\n",
      "Checking strides dimensions...\n",
      "conv_strides: (1, 1, 1, 1)\n",
      "pool_ksize: (1, 2, 2, 1)\n",
      "pool_strides (1, 1, 1, 1)\n",
      "batch_normalizer: <tensorflow.python.keras.layers.normalization.BatchNormalizationV1 object at 0x0000021D975792B0>\n",
      "conv_layer: (?, 19, 19, 32)\n",
      "conv2d_maxpool... End\n",
      "\n",
      "conv2d_maxpool... Start\n",
      "Checking inputs dimensions...\n",
      "conv_ksize: (5, 5)\n",
      "conv_num_outputs: 32\n",
      "Checking strides dimensions...\n",
      "conv_strides: (1, 1, 1, 1)\n",
      "pool_ksize: (1, 2, 2, 1)\n",
      "pool_strides (1, 1, 1, 1)\n",
      "batch_normalizer: <tensorflow.python.keras.layers.normalization.BatchNormalizationV1 object at 0x0000021D975F20F0>\n",
      "conv_layer: (?, 14, 14, 32)\n",
      "conv2d_maxpool... End\n",
      "\n",
      "conv2d_maxpool... Start\n",
      "Checking inputs dimensions...\n",
      "conv_ksize: (5, 5)\n",
      "conv_num_outputs: 64\n",
      "Checking strides dimensions...\n",
      "conv_strides: (1, 1, 1, 1)\n",
      "pool_ksize: (1, 2, 2, 1)\n",
      "pool_strides (1, 1, 1, 1)\n",
      "batch_normalizer: <tensorflow.python.keras.layers.normalization.BatchNormalizationV1 object at 0x0000021D974D3518>\n",
      "conv_layer: (?, 9, 9, 64)\n",
      "conv2d_maxpool... End\n",
      "\n",
      "conv2d_maxpool... Start\n",
      "Checking inputs dimensions...\n",
      "conv_ksize: (5, 5)\n",
      "conv_num_outputs: 64\n",
      "Checking strides dimensions...\n",
      "conv_strides: (1, 1, 1, 1)\n",
      "pool_ksize: (1, 2, 2, 1)\n",
      "pool_strides (1, 1, 1, 1)\n",
      "batch_normalizer: <tensorflow.python.keras.layers.normalization.BatchNormalizationV1 object at 0x0000021D975A1E80>\n",
      "conv_layer: (?, 4, 4, 64)\n",
      "conv2d_maxpool... End\n",
      "\n",
      "conv2d_maxpool... Start\n",
      "Checking inputs dimensions...\n",
      "conv_ksize: (2, 2)\n",
      "conv_num_outputs: 128\n",
      "Checking strides dimensions...\n",
      "conv_strides: (1, 1, 1, 1)\n",
      "pool_ksize: (1, 2, 2, 1)\n",
      "pool_strides (1, 1, 1, 1)\n",
      "batch_normalizer: <tensorflow.python.keras.layers.normalization.BatchNormalizationV1 object at 0x0000021D975794E0>\n",
      "conv_layer: (?, 2, 2, 128)\n",
      "conv2d_maxpool... End\n",
      "\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-8-f4cd7ea627b0>:73: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "def create_conv_net(x, keep_prob, n_classes):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data - normalized image \n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability \n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # Applying a few Convolution and Max Pool layers\n",
    "    \n",
    "    conv_ksize = (8, 8)  # filter dimensions\n",
    "    # conv_ksize = (3, 3)  # filter dimensions\n",
    "    conv_strides = (1, 1)\n",
    "    pool_ksize = (2, 2)  # Filter kernel/patch dimensions [batch, height, width, channels]\n",
    "    pool_strides = (1, 1)\n",
    "    \n",
    "    # batch_normalizer = tf.keras.layers.BatchNormalization(trainable=False)\n",
    "    # x = batch_normalizer(x)\n",
    "    \n",
    "    # batch_normalizer = None\n",
    "    \n",
    "    #conv_layer = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_num_outputs = 16  # D_out : number of out filters\n",
    "    batch_normalizer = tf.keras.layers.BatchNormalization(trainable=True)\n",
    "    conv_layer = cnn.conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides, \n",
    "                                    wieghts_name=\"weights-layer-1\", layer_name=\"hidden-layer-1\", batch_normalizer=batch_normalizer)\n",
    "    #conv_layer = normalize_batch(conv_layer)\n",
    "    \n",
    "    # next layer\n",
    "    conv_ksize = (5, 5)  # output layers dimensions\n",
    "    conv_num_outputs = 32\n",
    "    batch_normalizer = tf.keras.layers.BatchNormalization(trainable=True)\n",
    "    conv_layer = cnn.conv2d_maxpool(conv_layer, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides, \n",
    "                                    wieghts_name=\"weights-layer-2\", layer_name=\"hidden-layer-2\", batch_normalizer=batch_normalizer)\n",
    "    \n",
    "    # next layer\n",
    "    conv_ksize = (5, 5)  # output layers dimensions\n",
    "    conv_num_outputs = 32\n",
    "    batch_normalizer = tf.keras.layers.BatchNormalization(trainable=True)\n",
    "    conv_layer = cnn.conv2d_maxpool(conv_layer, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides, \n",
    "                                    wieghts_name=\"weights-layer-3\", layer_name=\"hidden-layer-3\", batch_normalizer=batch_normalizer)\n",
    "    \n",
    "    # next layer\n",
    "    conv_ksize = (5, 5)  # output layers dimensions\n",
    "    conv_num_outputs = 64\n",
    "    batch_normalizer = tf.keras.layers.BatchNormalization(trainable=True)\n",
    "    conv_layer = cnn.conv2d_maxpool(conv_layer, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides, \n",
    "                                    wieghts_name=\"weights-layer-4\", layer_name=\"hidden-layer-4\", batch_normalizer=batch_normalizer)\n",
    "    \n",
    "    # next layer\n",
    "    conv_ksize = (5, 5)  # output layers dimensions\n",
    "    conv_num_outputs = 64\n",
    "    batch_normalizer = tf.keras.layers.BatchNormalization(trainable=True)\n",
    "    conv_layer = cnn.conv2d_maxpool(conv_layer, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides, \n",
    "                                    wieghts_name=\"weights-layer-5\", layer_name=\"hidden-layer-5\", batch_normalizer=batch_normalizer)\n",
    "    \n",
    "    # next layer\n",
    "    conv_ksize = (2, 2)  # output layers dimensions\n",
    "    conv_num_outputs = 128\n",
    "    batch_normalizer = tf.keras.layers.BatchNormalization(trainable=True)\n",
    "    conv_layer = cnn.conv2d_maxpool(conv_layer, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides, \n",
    "                                    wieghts_name=\"weights-layer-6\", layer_name=\"hidden-layer-6\", batch_normalizer=batch_normalizer)\n",
    "    \n",
    "    ################################################\n",
    "    # flattenning the layer\n",
    "    x_tensor = cnn.flatten(conv_layer)\n",
    "\n",
    "    # Applying a few fully connected layers\n",
    "    \n",
    "    # x_tensor = tf.layers.batch_normalization(x_tensor)\n",
    "    x_tensor = cnn.fully_conn(x_tensor, 128)\n",
    "    # normalize_batch = tf.keras.layers.BatchNormalization(trainable=True)\n",
    "    # x_tensor = normalize_batch(x_tensor)\n",
    "    x_tensor = tf.nn.dropout(x_tensor, keep_prob)\n",
    "    \n",
    "    x_tensor = cnn.fully_conn(x_tensor, 64)\n",
    "    # normalize_batch = tf.keras.layers.BatchNormalization(trainable=True)\n",
    "    # x_tensor = normalize_batch(x_tensor)\n",
    "    x_tensor = tf.nn.dropout(x_tensor, keep_prob)\n",
    "\n",
    "    # Applying an Output Layer\n",
    "    output_tensor = cnn.output(x_tensor, n_classes)\n",
    "    \n",
    "    return output_tensor\n",
    "\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = cnn.neural_net_image_input((target_image_size, target_image_size, 3))\n",
    "y = cnn.neural_net_label_input(n_classes)\n",
    "keep_prob = cnn.neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = create_conv_net(x, keep_prob, n_classes)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name=\"logits\")\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name=\"accuracy\")\n",
    "\n",
    "# tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up hyper parameters and training on batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 512\n",
    "keep_probability = 0.80\n",
    "save_model_path = \"saved-model-01/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... start\n",
      "Epoch  1: Test Cost: 2.3501   ---   Valid Accuracy: 0.4488\n",
      "batch_index 25\r"
     ]
    }
   ],
   "source": [
    "print(\"Training... start\")\n",
    "\n",
    "test_costs = []\n",
    "valid_accuracies = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        batch_index = 0\n",
    "        \n",
    "        # for batch_features, batch_labels in CommonModules.load_preprocess_training_batch(batch_i, batch_size):\n",
    "        for batch_features, batch_labels in cnn.batch_features_labels(random_x_train_data, random_y_train_data, batch_size):\n",
    "            \n",
    "            print(\"batch_index\", batch_index, end=\"\\r\")\n",
    "            batch_index += 1\n",
    "            \n",
    "            cnn.train_neural_network(sess, x, y, keep_prob, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            \n",
    "        test_cost, valid_accuracy = cnn.print_stats(sess, x, y, keep_prob, batch_features, batch_labels, \n",
    "                                                    x_valid_data, y_valid_data, cost, accuracy, \"Epoch {0:>2}: \".format(epoch + 1))\n",
    "        \n",
    "        test_costs.append(test_cost)\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    "        \n",
    "        if((epoch + 1) % 1 == 0):\n",
    "            print(\"\")\n",
    "    \n",
    "    # Save the Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)\n",
    "\n",
    "print(\"Training... end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the performance \n",
    "\n",
    "- Measuring and plotting ROC - AUC curve of the model  \n",
    "- Visualizing the first five predictions of 10 samples from some test data set with the highest softmax scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = {}\n",
    "tpr = {}\n",
    "auc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from saved-model-01/\n",
      "Testing Accuracy: 0.947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_n_predictions = 5\n",
    "\n",
    "def test_model(test_features, test_labels, n_classes, n_samples):\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # test_images, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "    # loaded_graph = tf.get_default_graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "    # with tf.Session() as sess:\n",
    "        # sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + \".meta\")\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name(\"x:0\")\n",
    "        loaded_y = loaded_graph.get_tensor_by_name(\"y:0\")\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name(\"keep_prob:0\")\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name(\"logits:0\")\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name(\"accuracy:0\")\n",
    "        \n",
    "        # sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # loaded_y = tf.expand_dims(loaded_y, 1)\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in cnn.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            \n",
    "            test_batch_count += 1\n",
    "            \n",
    "        print(\"Testing Accuracy: {0:0.3f}\\n\".format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # ROC\n",
    "        #all_test_predictions = sess.run(tf.nn.top_k(tf.nn.softmax(loaded_logits), label_categories_count), \n",
    "        #                                            feed_dict={loaded_x: test_features, loaded_y: test_labels, loaded_keep_prob: 1.0})\n",
    "        \n",
    "        all_test_predictions = sess.run(tf.nn.softmax(loaded_logits), \n",
    "                                        feed_dict={loaded_x: test_features, loaded_y: test_labels, loaded_keep_prob: 1.0})\n",
    "\n",
    "        predicted_y_probabilities = (all_test_predictions / all_test_predictions.sum(axis=0, keepdims=1))[::, 0]\n",
    "\n",
    "        #print(\"all_test_predictions\", all_test_predictions[:10])\n",
    "        #print(\"y_pred_proba\", y_pred_proba[:10])\n",
    "\n",
    "        #y_pred = all_test_predictions[1][::, 0]\n",
    "        #y_pred_proba = (all_test_predictions[0] / all_test_predictions[0].sum(axis=0, keepdims=1))[::, 0]\n",
    "        \n",
    "        # print(\"y_pred_proba\", y_pred_proba)\n",
    "        \n",
    "        fpr[\"model-a\"], tpr[\"model-a\"], auc[\"model-a\"] = cnn.plot_roc_curve(test_labels[::, 0], predicted_y_probabilities, \n",
    "                                                                            title=\"Model-2\", legend_title=\"Model-2, auc\")\n",
    "        \n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        \n",
    "        # random_test_predictions = sess.run(tf.nn.softmax(loaded_logits),\n",
    "        #                                   feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        \n",
    "        #random_test_predictions = sess.run(tf.nn.top_k(random_test_predictions, label_categories_count))\n",
    "\n",
    "        random_test_predictions = sess.run(tf.nn.top_k(tf.nn.softmax(loaded_logits), n_classes), \n",
    "                                                       feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, \n",
    "                                                                  loaded_keep_prob: 1.0})\n",
    "        \n",
    "        # print(\"random_test_predictions\", random_test_predictions.values[0])\n",
    "        # print(\"random_test_labels\", np.asanyarray(random_test_labels).shape)\n",
    "        # print(\"random_test_labels\", cnn.lb.inverse_transform(np.asanyarray(random_test_labels)))\n",
    "        \n",
    "        # random_test_predictions = sess.run(\n",
    "        #  tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "        #  feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        cnn.display_image_predictions(random_test_features, random_test_labels, random_test_predictions, max_top_count=top_n_predictions)\n",
    "        \n",
    "    return (random_test_features, random_test_labels, random_test_predictions)\n",
    "        \n",
    "tested_features, tested_labels, tested_predictions = test_model(x_test_data, y_test_data, n_classes, n_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Visualize the Neural Network's State with Test Images  \n",
    "\n",
    " This Section is not required to complete but acts as an additional excersise for understaning the output of a neural network's weights. While neural networks can be a great learning device they are often referred to as a black box. We can understand what the weights of a neural network look like better by plotting their feature maps. After successfully training your neural network you can see what it's feature maps look like by plotting the output of the network's weight layers in response to a test stimuli image. From these plotted feature maps, it's possible to see what characteristics of an image the network finds interesting. For a sign, maybe the inner network feature maps react with high activation to the sign's boundary outline or to the contrast in the sign's painted symbol.\n",
    "\n",
    " Provided for you below is the function code that allows you to get the visualization output of any tensorflow weight layer you want. The inputs to the function should be a stimuli image, one used during training or a new one you provided, and then the tensorflow variable name that represents the layer's state during the training process, for instance if you wanted to see what the [LeNet lab's](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/601ae704-1035-4287-8b11-e2c2716217ad/concepts/d4aca031-508f-4e0b-b493-e7b706120f81) feature maps looked like for it's second convolutional layer you could enter conv2 as the tf_activation variable.\n",
    "\n",
    "For an example of what feature map outputs look like, check out NVIDIA's results in their paper [End-to-End Deep Learning for Self-Driving Cars](https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/) in the section Visualization of internal CNN State. NVIDIA was able to show that their network's inner weights had high activations to road boundary lines by comparing feature maps from an image with a clear path to one without. Try experimenting with a similar test to show that your trained network's weights are looking for interesting features, whether it's looking at differences in feature maps from images with or without a sign, or even what feature maps look like in a trained network vs a completely untrained one on the same sign image.\n",
    "\n",
    "<figure>\n",
    " <img src=\"visualize_cnn.png\" width=\"380\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Your output should look something like this (above)</p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saved-model-01/\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"The name 'hidden-layer-2:0' refers to a Tensor which does not exist. The operation, 'hidden-layer-2', does not exist in the graph.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-200-bf8ccb0a4cf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mconv_layer_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"weights-layer-1:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mconv_layer_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hidden-layer-2:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mconv_layer_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hidden-layer-3:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mconv_layer_4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hidden-layer-4:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mget_tensor_by_name\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   3652\u001b[0m       raise TypeError(\"Tensor names are strings (or similar), not %s.\" %\n\u001b[0;32m   3653\u001b[0m                       type(name).__name__)\n\u001b[1;32m-> 3654\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_graph_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3656\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_get_tensor_by_tf_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3477\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3478\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3480\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3518\u001b[0m           raise KeyError(\"The name %s refers to a Tensor which does not \"\n\u001b[0;32m   3519\u001b[0m                          \u001b[1;34m\"exist. The operation, %s, does not exist in the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3520\u001b[1;33m                          \"graph.\" % (repr(name), repr(op_name)))\n\u001b[0m\u001b[0;32m   3521\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3522\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mout_n\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"The name 'hidden-layer-2:0' refers to a Tensor which does not exist. The operation, 'hidden-layer-2', does not exist in the graph.\""
     ]
    }
   ],
   "source": [
    "### Visualize your network's feature maps here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "\"\"\"\n",
    "Test the saved model against the test dataset\n",
    "\"\"\"\n",
    "\n",
    "# test_images, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "loaded_graph = tf.Graph()\n",
    "# loaded_graph = tf.get_default_graph()\n",
    "\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # with tf.Session() as sess:\n",
    "    # sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Load model\n",
    "    loader = tf.train.import_meta_graph(save_model_path + \".meta\")\n",
    "    loader.restore(sess, save_model_path)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    loaded_x = loaded_graph.get_tensor_by_name(\"x:0\")\n",
    "    loaded_y = loaded_graph.get_tensor_by_name(\"y:0\")\n",
    "    loaded_keep_prob = loaded_graph.get_tensor_by_name(\"keep_prob:0\")\n",
    "    loaded_logits = loaded_graph.get_tensor_by_name(\"logits:0\")\n",
    "    loaded_acc = loaded_graph.get_tensor_by_name(\"accuracy:0\")\n",
    "\n",
    "    # loaded_graph.\n",
    "\n",
    "    conv_layer_1 = loaded_graph.get_tensor_by_name(\"hidden-layer-1:0\")\n",
    "    conv_layer_2 = loaded_graph.get_tensor_by_name(\"hidden-layer-2:0\")\n",
    "    conv_layer_3 = loaded_graph.get_tensor_by_name(\"hidden-layer-3:0\")\n",
    "    conv_layer_4 = loaded_graph.get_tensor_by_name(\"hidden-layer-4:0\")\n",
    "    conv_layer_5 = loaded_graph.get_tensor_by_name(\"hidden-layer-5:0\")\n",
    "\n",
    "    # \"weights-layer-1\"\n",
    "    # \"hidden-layer-1\"\n",
    "\n",
    "    image_input = test_features[0]\n",
    "\n",
    "    Cnn.output_feature_map(image_input, loaded_x, conv_layer_1, sess, activation_min=-1, activation_max=-1, plt_num=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_use = train_images[0].astype(np.float32)\n",
    "image_to_use = train_images[6].astype(np.float32)\n",
    "image_to_use = train_images[11].astype(np.float32)\n",
    "image_to_use = train_images[12].astype(np.float32)\n",
    "\n",
    "image_to_use = train_images[6].astype(np.float32)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "# loaded_graph = tf.get_default_graph()\n",
    "\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "\n",
    "    # Load model\n",
    "    loader = tf.train.import_meta_graph(save_model_path + \".meta\")\n",
    "    loader.restore(sess, save_model_path)\n",
    "    \n",
    "    loaded_x = loaded_graph.get_tensor_by_name(\"x:0\")\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    # loaded_weights_layer_1 = loaded_graph.get_tensor_by_name(\"weights-layer-1:0\").eval()\n",
    "    hidden_layer = loaded_graph.get_tensor_by_name(\"hidden-layer-3:0\")\n",
    "    keep_prob_tf = tf.placeholder(tf.float32, name=\"keep-prob-in\")\n",
    "    \n",
    "    # image_in_tf = tf.placeholder(tf.float32, [None, image_to_use.shape[0], image_to_use.shape[1], image_to_use.shape[2]], name=\"x\")\n",
    "    image_in_tf = loaded_x\n",
    "\n",
    "    filter_depth_count = hidden_layer.get_shape().as_list()[3]\n",
    "    activated_units = cnn.get_activations(sess, image_in_tf, keep_prob_tf, hidden_layer, image_to_use)\n",
    "    cnn.plot_nn_filter(activated_units, math.ceil(math.sqrt(filter_depth_count)))\n",
    "    \n",
    "    plt.subplots_adjust(wspace=-0.5, hspace=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
